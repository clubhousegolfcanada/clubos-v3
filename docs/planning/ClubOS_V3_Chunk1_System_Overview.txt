ClubOS V3 - Chunk 1: System Overview + Core Architecture

ğŸ“ Scope

Define the foundational architecture of ClubOS V3:
* Action-based, mobile-first operator interface
* Single-LLM agent per deployment with resilience and fallback strategies
* Modular routing to tools (Claude, OpenPhone, NinjaOne, DB, etc.)
* No chatbot behavior â€“ action-only logic, no multi-turn conversational flow
* Starts human-in-loop, builds toward autonomous trust via feedback loops
* Built-in rate limiting, cost controls, and error handling

ğŸ”„ Cross-Referenced Corrections
Early Plan                    â†’ Final Decision
4 LLMs (tech, booking, etc.) â†’ Single unified LLM per instance
Static SOP logic             â†’ AI-monitored, human-approved updates
Human escalation always manual â†’ Escalation logic is toggle-configurable
No error handling            â†’ Graceful degradation with fallbacks

ğŸ“¦ Dependencies
* OpenAI or Claude runtime w/ tool call support
* Railway + Vercel deployment stack (Node backend, Next.js frontend)
* NinjaOne, Ubiquiti, Skedda or Jason's booking API, OpenPhone
* Claude (or equivalent) for file parsing + SOP injection
* Rate limiting middleware and token tracking

ğŸ”§ Tasks

[1.1] Set System Identity and Mode
* Default LLM agent name: OperatorGPT
* Only one agent per deployment
* Must be able to:
  â—‹ Take customer messages
  â—‹ Route action requests (reset, unlock, etc.)
  â—‹ Search + invoke SOP logic
  â—‹ Call Claude for SOP parsing or feedback review
  â—‹ Handle errors gracefully with fallback to human

[1.2] Define LLM Autonomy Profile (Enhanced)
{
  "tech": {
    "autonomy": true,
    "max_fallbacks": 1,
    "confidence_threshold": 0.85,
    "require_booking": true,
    "time_restrictions": null,
    "cooldown_seconds": 900
  },
  "booking": {
    "autonomy": false,
    "confidence_threshold": 0.95,
    "require_booking": false,
    "time_restrictions": {"start": "08:00", "end": "22:00"}
  },
  "access": {
    "autonomy": true,
    "rules": ["within 10 min of booking", "no flags"],
    "confidence_threshold": 0.90,
    "max_attempts_per_booking": 2
  },
  "faq": {
    "autonomy": true,
    "confidence_threshold": 0.70,
    "use_cheaper_model": true
  }
}

[1.3] Implement Control Toggles & Error Handling
* Each autonomy behavior is controlled by:
  â—‹ Action category
  â—‹ Booking metadata
  â—‹ Confidence score
  â—‹ System health status
* Fallback logic is not hardcoded â€“ defined in config
* Error states:
  â—‹ LLM timeout â†’ Mark thread "awaiting_human"
  â—‹ API failure â†’ Log error, notify operator
  â—‹ Low confidence â†’ Escalate with reason
  â—‹ Rate limit hit â†’ Queue or defer action

[1.4] Claude Role (with Resilience)
* Claude is only used for:
  â—‹ SOP file ingestion
  â—‹ Conflict detection
  â—‹ Knowledge merge suggestions
  â—‹ Flagged feedback loop processing
* All Claude edits are flagged for human approval
* Claude offline handling:
  â—‹ SOP edit attempts deferred to queue
  â—‹ Operator notified of degraded service
  â—‹ System continues with cached SOPs

[1.5] Rate Limiting & Cost Controls
* Per-thread rate limits:
  â—‹ One LLM invocation per 15 seconds per thread
  â—‹ Configurable per deployment
* Per-action cooldowns:
  â—‹ Max 2 resets per bay per hour
  â—‹ Max 1 door unlock per booking (unless failed)
  â—‹ Cooldown violations trigger escalation
* Token management:
  â—‹ Max 500 tokens per response (configurable)
  â—‹ Daily/monthly caps per location
  â—‹ Model tiering:
    - GPT-4o for complex/high-confidence needs
    - GPT-3.5/Claude Instant for FAQ and lookups
    - Cached responses for repeated queries

[1.6] Load Distribution Preparation
* Single agent design optimized for:
  â—‹ Priority queue (emergency threads first)
  â—‹ FIFO processing with timeout handling
  â—‹ Future: parallel instances per location
  â—‹ Future: load balancer for multi-site ops

ğŸ¯ Expected Behavior
* System starts with OperatorGPT receiving all inbound messages
* No other assistants are used
* Based on message intent and context, the LLM:
  â—‹ Executes actions (if allowed and not rate-limited)
  â—‹ Escalates if configured to or on error
  â—‹ Logs all outcomes with performance metrics
* System degrades gracefully:
  â—‹ Falls back to human on errors
  â—‹ Queues non-urgent requests during high load
  â—‹ Uses cheaper models for simple queries
* Claude only invoked when:
  â—‹ New knowledge is uploaded
  â—‹ SOPs need review or cleanup
  â—‹ Human requests change
  â—‹ And system confirms Claude is available

proceed to Chunk 2: Message Flow, Thread State, and Escalation Logic.